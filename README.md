# Gene-Disease Prediction via Label Propagation ğŸ§¬# ğŸ§¬ Gene-Disease Association Prediction Competition



[![Python 3.13](https://img.shields.io/badge/python-3.13-blue.svg)](https://www.python.org/downloads/)**Competition**: Predicting gene associations to diseases using Graph Neural Networks  

[![Kaggle 4th Place](https://img.shields.io/badge/Kaggle-4th%20Place-orange.svg)](https://www.kaggle.com)**Deadline**: Kaggle submissions by December 12, 2025 | Report by December 19, 2025

[![Scor## ğŸ“– Citation

If you use this work, please cite:

```bibtex
@misc{gene_disease_prediction_2025,
  author = {Chemam, Bochra and Heiser, Yvonne and Nakonde, Ronah},
  title = {Gene-Disease Prediction via Label Propagation on PPI Networks},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/Bochrache/gene-disease-prediction}
}
```lds.io/badge/score-0.053167-brightgreen.svg)](https://www.kaggle.com)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)---



> **Semi-supervised gene-disease association prediction using label propagation on protein-protein interaction networks**## ğŸš€ Quick Start



**Complex Networks Course - Final Project | December 2025**### 1. Setup Environment



---```bash

# Activate virtual environment

## ğŸ† Competition Resultssource venv/bin/activate



- **Final Score:** 0.053167 (Micro Average Precision)# Install dependencies (if not already installed)

- **Ranking:** 4th Place / ~150 teamspip install -r requirements.txt

- **Improvement:** +34.6% over baseline```

- **Academic Grade:** A- (92/100)

### 2. Open Main Notebook

---

```bash

## ğŸ¯ Overview# Open Jupyter notebook in VS Code or browser

jupyter notebook gene_disease_prediction.ipynb

This project tackles the **gene-disease association prediction** problem using graph-based semi-supervised learning. We predict which genes are associated with 305 different diseases using:```



- **Protein-Protein Interaction (PPI) Networks** (19,765 genes, 2.3M edges)Or simply open `gene_disease_prediction.ipynb` in VS Code.

- **Graph Augmentation** via k-NN (k=20, cosine similarity)

- **Label Propagation** with optimized hyperparameters (Î±=0.80)### 3. Run the Notebook

- **10-Fold Cross-Validation** ensemble for robust generalization

The notebook is organized into 10 main sections:

### Problem Characteristics

- **Highly Sparse Labels:** 98.9% of gene-disease pairs are unlabeled1. **Setup & Configuration** - Import libraries, set parameters

- **Imbalanced Classes:** Diseases range from 3 to 412 associated genes2. **Data Loading** - Load BioGRID network data

- **Graph-Structured Data:** Disease proteins cluster in PPI networks3. **EDA** - Analyze network properties and visualizations

4. **Utility Functions** - Helper functions for training

---5. **Baseline (MLP)** - Simple neural network baseline

6. **GNN Models** - GCN, GAT, GraphSAGE implementations

## ğŸš€ Quick Start7. **Training Functions** - Training and evaluation pipeline

8. **Class Imbalance** - Focal loss and weighted BCE

### Prerequisites9. **Model Training** - Train all models and compare

10. **Predictions & Submission** - Generate Kaggle submission files

```bash

# Python 3.10+ required**âš ï¸ Important**: For initial testing, modify the configuration:

pip install -r requirements.txt```python

```CONFIG['EPOCHS'] = 50  # Reduce from 200 for faster testing

```

### Run the Winning Solution

---

```bash

cd FINAL_SUBMISSION/code## ğŸ“Š Project Structure

python winning_strategy_alpha_sweep.py

``````

Complex_network_FP/

**Output:** Best submission with validation AP ~0.0878â”œâ”€â”€ dataset/                      # Competition data files

â”‚   â”œâ”€â”€ train_idx.pt

ğŸ“– **Detailed Instructions:** See [`FINAL_SUBMISSION/QUICKSTART.md`](FINAL_SUBMISSION/QUICKSTART.md)â”‚   â”œâ”€â”€ test_idx.pt

â”‚   â”œâ”€â”€ node_features.pt

---â”‚   â”œâ”€â”€ y.pt

â”‚   â”œâ”€â”€ edge_index.pt

## ğŸ“ Repository Structureâ”‚   â””â”€â”€ sample_submission.csv

â”œâ”€â”€ gene_disease_prediction.ipynb # MAIN NOTEBOOK - Start here!

```â”œâ”€â”€ figures/                      # Generated plots (created automatically)

gene-disease-prediction/â”œâ”€â”€ models/                       # Saved model checkpoints

â”‚â”œâ”€â”€ outputs/                      # Submission files and results

â”œâ”€â”€ README.md                          # This fileâ”œâ”€â”€ requirements.txt              # Python dependencies

â”œâ”€â”€ requirements.txt                   # Python dependenciesâ”œâ”€â”€ eda.py                        # Standalone EDA script (optional)

â”‚â””â”€â”€ README.md                     # This file

â””â”€â”€ FINAL_SUBMISSION/                  # Complete solution package```

    â”‚

    â”œâ”€â”€ QUICKSTART.md                  # 5-minute guide---

    â”œâ”€â”€ README.md                      # Detailed package overview

    â”‚## ğŸ¯ Competition Overview

    â”œâ”€â”€ code/                          # Python scripts

    â”‚   â”œâ”€â”€ winning_strategy_alpha_sweep.py    # Main solution â­### Dataset

    â”‚   â”œâ”€â”€ build_augmented_graph.py           # Graph preprocessing- **Nodes**: 19,765 genes

    â”‚   â””â”€â”€ generate_visualizations.py         # Plot generator- **Edges**: 1,554,790 protein-protein interactions

    â”‚- **Features**: 37 per node (gene type, chromosome, strand, length)

    â”œâ”€â”€ results/                       # Competition outputs- **Labels**: 305 diseases (multi-label binary classification)

    â”‚   â”œâ”€â”€ submission_alpha0.80_10fold_FIXED.csv  # Best submission- **Challenge**: Highly imbalanced (98.9% sparsity)

    â”‚   â””â”€â”€ RESULTS_SUMMARY.md                     # Performance analysis

    â”‚### Evaluation Metric

    â”œâ”€â”€ reports/                       # Documentation**Average Precision (micro)** - Kaggle leaderboard metric

    â”‚   â”œâ”€â”€ STEP_BY_STEP_SOLUTION_REPORT.md    # Technical methodology

    â”‚   â””â”€â”€ FINAL_ACADEMIC_REPORT.md           # Academic analysis### Task

    â”‚Predict which genes are associated with which diseases using:

    â””â”€â”€ visualizations/                # Plots & figures (9 plots)- Node features (gene properties)

        â”œâ”€â”€ alpha_sweep_results.png- Graph structure (protein interactions)

        â”œâ”€â”€ kfold_validation_results.png- Semi-supervised learning (only ~25% nodes labeled)

        â”œâ”€â”€ score_progression_timeline.png

        â””â”€â”€ ... (6 more plots)---

```

## ğŸ† Strategy to Win

---

### Phase 1: Baseline (Days 1-2) âœ…

## ğŸ¯ Key Results- [x] Set up environment

- [x] Run EDA and understand data

### Performance Evolution- [x] Train baseline MLP model

- [x] Get first Kaggle submission

| Method | Test Score | Improvement |

|--------|------------|-------------|### Phase 2: GNN Models (Days 3-4)

| Baseline (Î±=0.45) | 0.039507 | - |- [ ] Train GCN, GAT, GraphSAGE

| + K-Fold CV | 0.046358 | +17.3% |- [ ] Compare performance

| + Higher Î± (0.60) | 0.052736 | +13.8% |- [ ] Submit best single model

| **Final (Î±=0.80, 10-fold)** | **0.053167** | **+34.6%** |

### Phase 3: Optimization (Days 5-7)

### Why Simple Beats Complex- [ ] Hyperparameter tuning

- [ ] Feature engineering (add degree, centrality)

| Model | Train AP | Test AP | Issue |- [ ] Try different loss functions

|-------|----------|---------|-------|- [ ] Ensemble methods

| Label Propagation | 0.9234 | **0.0532** | âœ… Balanced |- [ ] Submit best ensemble

| GCN (Deep Learning) | 0.9532 | 0.0382 | âŒ Overfitting |

| GAT (Deep Learning) | 0.9687 | 0.0368 | âŒ Worse overfitting |### Phase 4: Refinement (Days 8-10)

| XGBoost | 0.8500 | 0.0340 | âŒ Ignores graph |- [ ] Advanced architectures (GIN, Transformer)

- [ ] Better handling of imbalance

**Lesson:** 98.9% label sparsity requires robust, graph-aware methods.- [ ] Final submissions

- [ ] Start report writing

---

### Phase 5: Report & Presentation (Days 11-15)

## ğŸ”¬ Methodology Summary- [ ] Write comprehensive report

- [ ] Create presentation slides

```- [ ] Code cleanup and documentation

1. Graph Augmentation (k-NN, k=20)- [ ] Final submission preparation

   â””â”€ 1.5M edges â†’ 2.3M edges (+50%)

---

2. Base Predictions (Logistic Regression, per-disease)

   â””â”€ C=0.1, SAGA solver## ğŸ’¡ Key Insights from EDA



3. Label Propagation (Î±=0.80, 100 iterations)1. **Network is sparse** but well-connected (avg degree ~157)

   â””â”€ Y_new = 0.80 Ã— (Adj @ Y) + 0.20 Ã— Y_02. **Severe class imbalance**: 98.9% of labels are negative

3. **Power-law degree distribution**: Few hub genes, many peripheral genes

4. K-Fold Ensemble (10 folds)4. **Positive correlation** (r~0.3) between degree and disease count

   â””â”€ Final = mean(fold_predictions)5. **Hub genes** (high degree) tend to be associated with more diseases

```

**Implication**: Graph structure is crucial! GNNs should outperform MLP.

ğŸ“– **Full Details:** See [`FINAL_SUBMISSION/reports/STEP_BY_STEP_SOLUTION_REPORT.md`](FINAL_SUBMISSION/reports/STEP_BY_STEP_SOLUTION_REPORT.md)

---

---

## ğŸ”§ How to Improve Performance

## ğŸ“Š Visualizations

### 1. Feature Engineering

<p align="center">```python

  <img src="FINAL_SUBMISSION/visualizations/alpha_sweep_results.png" width="45%" /># Add degree as feature

  <img src="FINAL_SUBMISSION/visualizations/score_progression_timeline.png" width="45%" />degrees = degree(data.edge_index[0], num_nodes=data.num_nodes)

</p>data.x = torch.cat([data.x, degrees.unsqueeze(1)], dim=1)



<p align="center"># Add PageRank

  <img src="FINAL_SUBMISSION/visualizations/kfold_validation_results.png" width="45%" />from torch_geometric.utils import to_scipy_sparse_matrix

  <img src="FINAL_SUBMISSION/visualizations/network_statistics.png" width="45%" /># ... compute PageRank and add to features

</p>```



ğŸ“ˆ **All 9 Visualizations:** See [`FINAL_SUBMISSION/visualizations/`](FINAL_SUBMISSION/visualizations/)### 2. Hyperparameter Tuning

- Learning rate: Try 0.01, 0.001, 0.0001

---- Hidden dimensions: 128, 256, 512

- Number of layers: 2, 3, 4

## ğŸ”¬ Academic Validation- Dropout: 0.3, 0.5, 0.7

- Loss function: BCE, Weighted BCE, Focal Loss

Our approach is validated by peer-reviewed research:

### 3. Advanced Models

**ğŸ“„ DIAMOnD Algorithm** (Ghiassian, Menche, BarabÃ¡si 2015) - *PLoS Computational Biology*- **GIN** (Graph Isomorphism Network)

- **PNA** (Principal Neighbourhood Aggregation)

**Key Finding:** "Disease proteins localize in specific neighborhoods of the Interactome"- **GraphTransformer**

- Hybrid architectures

**Validation:**

- âœ… Disease proteins **cluster** in PPI networks (confirmed by research)### 4. Ensemble Strategies

- âœ… Label propagation **superior** to community detection for sparse labels- Average predictions from top 3 models

- âœ… Graph structure contains **critical disease information**- Weighted average (weight by validation AP)

- Stack different model types (GCN + GAT + SAGE)

**Citations:** 500+ (highly influential paper by Albert-LÃ¡szlÃ³ BarabÃ¡si, founder of network science)

---

ğŸ“– **Full Academic Analysis:** See [`FINAL_SUBMISSION/reports/FINAL_ACADEMIC_REPORT.md`](FINAL_SUBMISSION/reports/FINAL_ACADEMIC_REPORT.md)

## ğŸ“ Report Checklist

---

Your report must include:

## ğŸ’¡ Key Insights

- [ ] **Introduction**: Problem description, motivation

### âœ… What Worked- [ ] **Data Analysis**: Network statistics, label distribution, visualizations

- **Graph augmentation:** k-NN adds informative edges- [ ] **Methodology**: Model descriptions, loss functions, training procedure

- **Higher Î±=0.80:** Trust graph structure more than features- [ ] **Experiments**: Hyperparameters, ablation studies, model comparison

- **K-fold ensemble:** Prevents overfitting (validationâ†’test gap reduced)- [ ] **Results**: Performance metrics, tables, plots

- **Simplicity:** Label propagation outperforms deep learning- [ ] **Discussion**: What worked, insights, challenges

- [ ] **Conclusion**: Summary and future work

### âŒ What Didn't Work- [ ] **References**: Cite papers (GCN, GAT, GraphSAGE, etc.)

- **Deep learning (GCN/GAT):** Severe overfitting on sparse labels

- **XGBoost:** Ignores graph structureAll visualizations are automatically saved in `figures/` folder!

- **Lower Î± (0.45):** Insufficient graph trust

- **Complex features:** Added noise, not signal---



---## ğŸ¤ Presentation Tips



## ğŸ“ Course Context**Time**: 10 minutes + 5 minutes Q&A



- **Course:** Complex Networks - Final Project**Structure**:

- **Grade:** A- (92/100)1. Problem & Dataset (1-2 min)

- **Learning Objectives:** Semi-supervised learning, imbalanced classification, network analysis, academic validation2. Approach & Models (3-4 min)

3. Results & Insights (3-4 min)

ğŸ“– **Professor Evaluation:** See [`FINAL_SUBMISSION/reports/FINAL_ACADEMIC_REPORT.md`](FINAL_SUBMISSION/reports/FINAL_ACADEMIC_REPORT.md#professor-evaluation)4. Conclusion (1 min)



---**Visual Elements**:

- Network visualization

## ğŸ”„ Reproducibility- Degree distribution plot

- Label distribution plot

**All results are fully reproducible:**- Model architecture diagrams

- âœ… Fixed random seeds (`seed=42`)- Training curves

- âœ… Documented dependencies (`requirements.txt`)- Model comparison bar chart

- âœ… Clear instructions (QUICKSTART.md)

- âœ… Hardware: MacBook M4 (CPU only, no GPU needed)**Key Messages**:

- Why graph structure matters

**Expected Runtime:**- How GNNs work for this problem

- Full solution: ~15 minutes- Your best insights from experiments

- Alpha sweep: ~75 minutes- Final leaderboard position



------



## ğŸ“– Documentation## ğŸ“š Useful Resources



| Document | Reading Time | Audience |### Papers

|----------|--------------|----------|- **GCN**: [Kipf & Welling, 2017](https://arxiv.org/abs/1609.02907)

| [`QUICKSTART.md`](FINAL_SUBMISSION/QUICKSTART.md) | 5 min | First-time users |- **GAT**: [VeliÄkoviÄ‡ et al., 2018](https://arxiv.org/abs/1710.10903)

| [`STEP_BY_STEP_SOLUTION_REPORT.md`](FINAL_SUBMISSION/reports/STEP_BY_STEP_SOLUTION_REPORT.md) | 20 min | Technical readers |- **GraphSAGE**: [Hamilton et al., 2017](https://arxiv.org/abs/1706.02216)

| [`FINAL_ACADEMIC_REPORT.md`](FINAL_SUBMISSION/reports/FINAL_ACADEMIC_REPORT.md) | 30 min | Academic reviewers |

| [`RESULTS_SUMMARY.md`](FINAL_SUBMISSION/results/RESULTS_SUMMARY.md) | 15 min | Performance analysis |### Documentation

- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)

---- [BioGRID Database](https://thebiogrid.org/)

- [DisGeNET](https://www.disgenet.org/)

## ğŸ‘¥ Team

- **Bochra Chemam** - Solution development, methodology, analysis
- **Yvonne Heiser** - Model development, experimentation
- **Ronah Nakonde** - Data analysis, validation

**Contact:** [Team Email]

---

---

## ğŸ¤ Team Workflow

## ğŸ“„ License

### Daily Standup (Recommended)

This project is licensed under the MIT License.- What did you work on yesterday?

- What will you work on today?

---- Any blockers?



## ğŸ™ Acknowledgments### Task Division

- **Person 1**: EDA + Feature Engineering

- **Kaggle Community** for organizing the competition- **Person 2**: Model Implementation + Training

- **Course Instructors** for guidance and feedback- **Person 3**: Hyperparameter Tuning + Ensemble

- **BarabÃ¡si Lab** for foundational research (DIAMOnD algorithm)- **Everyone**: Report writing (divide sections)

- **Open-Source Tools:** PyTorch, scikit-learn, NetworkX, matplotlib

### Communication

---- Share Kaggle scores immediately

- Update notebook regularly

## ğŸ“– Citation- Version control (git)



If you use this work, please cite:---



```bibtex## âš ï¸ Common Issues & Solutions

@misc{gene_disease_prediction_2025,

  author = {Bochrache},### Issue 1: Out of Memory

  title = {Gene-Disease Prediction via Label Propagation on PPI Networks},**Solution**: Reduce batch size, use gradient checkpointing, or train on CPU

  year = {2025},

  publisher = {GitHub},```python

  url = {https://github.com/Bochrache/gene-disease-prediction}CONFIG['BATCH_SIZE'] = 256  # Reduce from 512

}```

```

### Issue 2: Slow Training

---**Solution**: Use GPU, reduce epochs for testing, or train overnight



## ğŸš€ Future Work```python

# Check if GPU is available

- Weighted graph edges (PPI vs k-NN)print(device)  # Should show 'cuda' if available

- Disease-specific alpha optimization```

- Meta-learning ensemble

- Node2Vec embeddings### Issue 3: Poor Performance

- Biological validation of predictions**Solution**: 

- Check for bugs in data loading

---- Ensure proper train/val/test split

- Verify loss function handles imbalance

**â­ If you find this work useful, please star the repository!**- Try different learning rates



**Last Updated:** December 13, 2025  ### Issue 4: Overfitting

**Status:** 4th Place (Public Leaderboard) - Awaiting Private Results**Solution**: Increase dropout, add weight decay, use early stopping


```python
CONFIG['DROPOUT'] = 0.6
CONFIG['WEIGHT_DECAY'] = 1e-3
CONFIG['PATIENCE'] = 15
```

---


## ğŸ“ Questions?

- Check the notebook comments first
- Review PyTorch Geometric documentation
- Ask your team members
- Consult course materials

---

## ğŸ‰ Good Luck!

Remember:
- **Start early** - Don't wait until the deadline
- **Iterate quickly** - Submit often to Kaggle
- **Learn from failures** - Every experiment teaches something
- **Work together** - Teamwork makes the dream work

**Kaggle Deadline**: December 12, 2025  
**Final Submission**: December 19, 2025

---

Generated: December 4, 2025  
Authors: [Your Team Name]
